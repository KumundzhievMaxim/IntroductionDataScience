{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide: \n",
    " - data explonatory \n",
    " - data insights\n",
    " - features selection \n",
    " - transforming dataset into required format (for further modelS applies)\n",
    "\n",
    "### TODO\n",
    "- find empty columns <-- there is no missing values\n",
    "\n",
    "## Assumed steps\n",
    "#### Benchmark\n",
    "    - dataset_v0\n",
    "        # EVALUATE \n",
    "        - [catboost_v0] untuned classifier on dataset_v0 data set (~75% accuracy)\n",
    "        - [catboost_v1] tuned classifier on dataset_v0 data set\n",
    "        \n",
    "####  Data set Preprocessing\n",
    "    - dataset_v1\n",
    "        - data cleaning\n",
    "            - ignore the tuples \n",
    "            - missing values\n",
    "                - fill with mean | meadian | custom\n",
    "            - elinimate noise data \n",
    "                - binning method & regression & clustering \n",
    "        - data transformation\n",
    "            - normalization\n",
    "            - attribute selection\n",
    "            - discretization\n",
    "        - data reduction\n",
    "        \n",
    "\n",
    "### CAVEATS \n",
    "- Imbalanced target (700 vs 300)\n",
    "- Odd mean loan of creditors age of > 65 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Rename columns names for explicit visual understanding of features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    @staticmethod\n",
    "    def rename_columns(dataset: pd.DataFrame):\n",
    "        \"\"\"Rename dataframe columns names.\n",
    "        Notes:\n",
    "            target column names based on provided data_description.txt\n",
    "        Returns:\n",
    "            pd.DataFrame - dataframe with renamed columns \n",
    "        \"\"\"\n",
    "    \n",
    "        target_columns = {\n",
    "            'X01': 'account_status',\n",
    "            'X06': 'account_savings',\n",
    "            'X02': 'credit_duration',\n",
    "            'X03': 'credit_history',\n",
    "            'X04': 'credit_purpose',\n",
    "            'X05': 'credit_amount',\n",
    "            'X07': 'employment_status',\n",
    "            'X17': 'employment_description',\n",
    "            'X08': 'income_installment_rate',\n",
    "            'X09': 'gender_status',\n",
    "            'X10': 'credit_guarantors',\n",
    "            'X11': 'residence',\n",
    "            'X12': 'owned_property',\n",
    "            'X13': 'age',\n",
    "            'X14': 'installment_plans',\n",
    "            'X15': 'accomondation_type',\n",
    "            'X16': 'credit_existing_number',\n",
    "            'X18': 'liable_maintain',\n",
    "            'X19': 'phone_number',\n",
    "            'X20': 'foreign_worker',\n",
    "            'Y': 'y'\n",
    "        }\n",
    "        return dataset.rename(columns=target_columns)    \n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_columns(dataset: pd.DataFrame):\n",
    "        \"\"\"Within dataset find categorical columns\n",
    "        \n",
    "        Returns:\n",
    "          List: list of names of categorical columns.\n",
    "        \"\"\"\n",
    "        categorical_columns_indexes = []\n",
    "        \n",
    "        columns = dataset.columns\n",
    "        num_columns = dataset._get_numeric_data().columns # get numerical columns\n",
    "        \n",
    "        categorical_columns = sorted(list(set(columns) - set(num_columns)))\n",
    "        categorical_columns_indexes = [dataset.columns.get_loc(column) for column in categorical_columns]\n",
    "        return categorical_columns, categorical_columns_indexes\n",
    "          \n",
    "    @staticmethod\n",
    "    def train_test_split(dataset: pd.DataFrame, test_size: int, shuffle=False):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        print(f'Splitting dataset {dataset.shape} with test_size :{test_size}')\n",
    "        X = dataset.drop(columns=['y'])\n",
    "        y = dataset['y']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=shuffle, test_size=test_size)\n",
    "        print(f'X_train: {X_train.shape}')\n",
    "        print(f'y_train: {y_train.shape}')\n",
    "        print(f'X_test: {X_test.shape}')\n",
    "        print(f'y_test: {y_test.shape}')\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    @staticmethod\n",
    "    def custom_split(dataset: pd.DataFrame, test_percentage: int):\n",
    "        size = dataset.shape[0]\n",
    "        train_ratio = (size * test_percentage) // 100\n",
    "        test_ratio = size - train_ratio\n",
    "\n",
    "        X_train = pd.DataFrame(dataset[:train_ratio, :-1])\n",
    "        X_test = pd.DataFrame(dataset[:test_ratio, :-1])\n",
    "\n",
    "        y_train = dataset[:train_ratio, -1]\n",
    "        y_test = dataset[:test_ratio, -1]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_cleaning(dataframe: pd.DataFrame):\n",
    "        \"\"\"Applies follwoing transformations:   \n",
    "            - data cleaning\n",
    "                - ignore the tuples # when the dataset is large and multiple values are missing within a tuple.\n",
    "                - missing values\n",
    "                    - fill with mean | meadian | custom\n",
    "                - encoding\n",
    "        \"\"\"\n",
    "        categorical_columns = [column for column in dataset_v1.columns if dataset_v1[f'{column}'].dtype == 'object']\n",
    "    \n",
    "        for column_name in categorical_columns: \n",
    "            labels = dataframe[f'{column_name}'].astype('category').cat.categories.tolist()\n",
    "            replace_map_comp = {f'{column_name}' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "            dataframe.replace(replace_map_comp, inplace=True)\n",
    "        return dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_transformation(dataframe: pd.DataFrame, features=20):\n",
    "        \"\"\"Applies follwoing transformations:\n",
    "            - data transformation\n",
    "                - normalization # preprocessing.MinMaxScaler()\n",
    "                - attribute selection # SelectKBest(chi2, k=features)\n",
    "                - discretization # muted for now \n",
    "        \"\"\"\n",
    "        from sklearn import preprocessing\n",
    "        from sklearn.feature_selection import chi2\n",
    "        from sklearn.feature_selection import SelectKBest\n",
    "        original_columns = ['account_status', 'credit_duration', 'credit_history', 'credit_purpose', 'credit_amount', 'account_savings',\n",
    "                            'employment_status', 'income_installment_rate', 'gender_status','credit_guarantors', 'residence', 'owned_property', 'age',\n",
    "                            'installment_plans', 'accomondation_type', 'credit_existing_number', 'employment_description',\n",
    "                            'liable_maintain', 'phone_number', 'foreign_worker', 'y'\n",
    "                           ]\n",
    "        \n",
    "        values = dataframe.values #returns a numpy array\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(values)\n",
    "        buff_df = pd.DataFrame(x_scaled)\n",
    "        \n",
    "        buff_df.columns = original_columns # cast back original column namings \n",
    "        \n",
    "        X = buff_df.drop(columns=['y'], axis=1)\n",
    "        y = buff_df['y']\n",
    "        \n",
    "        dataframe = SelectKBest(chi2, k=features).fit_transform(X, y)\n",
    "        return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    @staticmethod\n",
    "    def _catboost_classifier(X_train, X_test, y_train, y_test, cat_features, **kwargs):\n",
    "        \"\"\"CatBoost Classifier.\n",
    "        \n",
    "        Notes:\n",
    "          - cat_features: CatBoost model requires list of indexes which denote categorical columns;\n",
    "        \"\"\"\n",
    "        \n",
    "        from catboost import CatBoostClassifier, Pool\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        train_data = Pool(\n",
    "            data=X_train.values,\n",
    "            label=y_train,\n",
    "            cat_features=cat_features\n",
    "        )\n",
    "        model = CatBoostClassifier(iterations=30)\n",
    "        model.fit(train_data)\n",
    "        predictions = model.predict(X_test.values)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        return accuracy\n",
    "    \n",
    "    @staticmethod\n",
    "    def _xgboost_classifier(X_train, X_test, y_train, y_test, **kwargs):\n",
    "        \"\"\"Apply XGBoost Classifier.\n",
    "        Notes:\n",
    "          When using XGBoost we need to convert categorical variables into numeric.\n",
    "        \"\"\"\n",
    "        from xgboost import XGBClassifier\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        model = XGBClassifier()\n",
    "        model.fit(X_train, y_train, verbose=True)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset (1000, 21) with test_size :0.3\n",
      "X_train: (700, 20)\n",
      "y_train: (700,)\n",
      "X_test: (300, 20)\n",
      "y_test: (300,)\n",
      "Learning rate set to 0.22043\n",
      "0:\tlearn: 0.6143646\ttotal: 4.14ms\tremaining: 120ms\n",
      "1:\tlearn: 0.5655417\ttotal: 7.62ms\tremaining: 107ms\n",
      "2:\tlearn: 0.5255207\ttotal: 11.1ms\tremaining: 100ms\n",
      "3:\tlearn: 0.5003964\ttotal: 14.4ms\tremaining: 93.6ms\n",
      "4:\tlearn: 0.4860743\ttotal: 18.5ms\tremaining: 92.7ms\n",
      "5:\tlearn: 0.4671726\ttotal: 21.8ms\tremaining: 87.4ms\n",
      "6:\tlearn: 0.4503195\ttotal: 25.2ms\tremaining: 82.7ms\n",
      "7:\tlearn: 0.4395020\ttotal: 28.4ms\tremaining: 78ms\n",
      "8:\tlearn: 0.4348984\ttotal: 31.3ms\tremaining: 73.1ms\n",
      "9:\tlearn: 0.4243975\ttotal: 34.2ms\tremaining: 68.5ms\n",
      "10:\tlearn: 0.4171205\ttotal: 37.6ms\tremaining: 64.9ms\n",
      "11:\tlearn: 0.4071788\ttotal: 40.5ms\tremaining: 60.7ms\n",
      "12:\tlearn: 0.4006725\ttotal: 43.8ms\tremaining: 57.3ms\n",
      "13:\tlearn: 0.3961232\ttotal: 46.8ms\tremaining: 53.5ms\n",
      "14:\tlearn: 0.3920035\ttotal: 49.9ms\tremaining: 49.9ms\n",
      "15:\tlearn: 0.3870953\ttotal: 53.1ms\tremaining: 46.5ms\n",
      "16:\tlearn: 0.3780961\ttotal: 56ms\tremaining: 42.8ms\n",
      "17:\tlearn: 0.3675973\ttotal: 59.5ms\tremaining: 39.7ms\n",
      "18:\tlearn: 0.3569638\ttotal: 63.1ms\tremaining: 36.5ms\n",
      "19:\tlearn: 0.3505955\ttotal: 66.8ms\tremaining: 33.4ms\n",
      "20:\tlearn: 0.3465541\ttotal: 69.6ms\tremaining: 29.8ms\n",
      "21:\tlearn: 0.3447232\ttotal: 72.3ms\tremaining: 26.3ms\n",
      "22:\tlearn: 0.3341795\ttotal: 74.7ms\tremaining: 22.7ms\n",
      "23:\tlearn: 0.3339704\ttotal: 77ms\tremaining: 19.2ms\n",
      "24:\tlearn: 0.3272642\ttotal: 79.4ms\tremaining: 15.9ms\n",
      "25:\tlearn: 0.3218086\ttotal: 81.9ms\tremaining: 12.6ms\n",
      "26:\tlearn: 0.3157340\ttotal: 85ms\tremaining: 9.44ms\n",
      "27:\tlearn: 0.3094401\ttotal: 87.9ms\tremaining: 6.28ms\n",
      "28:\tlearn: 0.3049143\ttotal: 90.8ms\tremaining: 3.13ms\n",
      "29:\tlearn: 0.2992421\ttotal: 93.8ms\tremaining: 0us\n",
      "Learning rate set to 0.153513\n",
      "0:\tlearn: 0.6033199\ttotal: 597us\tremaining: 17.3ms\n",
      "1:\tlearn: 0.5310919\ttotal: 1.23ms\tremaining: 17.2ms\n",
      "2:\tlearn: 0.4880494\ttotal: 1.88ms\tremaining: 16.9ms\n",
      "3:\tlearn: 0.4375554\ttotal: 2.57ms\tremaining: 16.7ms\n",
      "4:\tlearn: 0.3923795\ttotal: 3.31ms\tremaining: 16.6ms\n",
      "5:\tlearn: 0.3533820\ttotal: 3.67ms\tremaining: 14.7ms\n",
      "6:\tlearn: 0.3281389\ttotal: 4.2ms\tremaining: 13.8ms\n",
      "7:\tlearn: 0.3061157\ttotal: 4.66ms\tremaining: 12.8ms\n",
      "8:\tlearn: 0.2888455\ttotal: 5.18ms\tremaining: 12.1ms\n",
      "9:\tlearn: 0.2745182\ttotal: 5.71ms\tremaining: 11.4ms\n",
      "10:\tlearn: 0.2598103\ttotal: 6.21ms\tremaining: 10.7ms\n",
      "11:\tlearn: 0.2455275\ttotal: 6.76ms\tremaining: 10.1ms\n",
      "12:\tlearn: 0.2352901\ttotal: 7.31ms\tremaining: 9.56ms\n",
      "13:\tlearn: 0.2279832\ttotal: 7.84ms\tremaining: 8.96ms\n",
      "14:\tlearn: 0.2179746\ttotal: 8.4ms\tremaining: 8.4ms\n",
      "15:\tlearn: 0.2093070\ttotal: 8.91ms\tremaining: 7.8ms\n",
      "16:\tlearn: 0.2043582\ttotal: 9.43ms\tremaining: 7.21ms\n",
      "17:\tlearn: 0.2000408\ttotal: 9.9ms\tremaining: 6.6ms\n",
      "18:\tlearn: 0.1952684\ttotal: 10.4ms\tremaining: 6ms\n",
      "19:\tlearn: 0.1891042\ttotal: 10.9ms\tremaining: 5.45ms\n",
      "20:\tlearn: 0.1856426\ttotal: 11.4ms\tremaining: 4.88ms\n",
      "21:\tlearn: 0.1828130\ttotal: 11.9ms\tremaining: 4.34ms\n",
      "22:\tlearn: 0.1763039\ttotal: 12.4ms\tremaining: 3.78ms\n",
      "23:\tlearn: 0.1719193\ttotal: 13.2ms\tremaining: 3.31ms\n",
      "24:\tlearn: 0.1682581\ttotal: 13.9ms\tremaining: 2.78ms\n",
      "25:\tlearn: 0.1635148\ttotal: 14.4ms\tremaining: 2.22ms\n",
      "26:\tlearn: 0.1613630\ttotal: 15.1ms\tremaining: 1.68ms\n",
      "27:\tlearn: 0.1571260\ttotal: 15.8ms\tremaining: 1.13ms\n",
      "28:\tlearn: 0.1539175\ttotal: 16.5ms\tremaining: 567us\n",
      "29:\tlearn: 0.1521636\ttotal: 17.1ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "models = Models()\n",
    "transformer = Transformer()\n",
    "\n",
    "\n",
    "row_data = pd.read_csv('./dataset/project_data.csv', delimiter=';')\n",
    "# print(row_data)\n",
    "\n",
    "\n",
    "# # Benchmark\n",
    "dataset_v0 = transformer.rename_columns(row_data)\n",
    "categorical_columns, categorical_columns_indexes = transformer.categorical_columns(dataset_v0) \n",
    "X_train_v0, X_test_v0, y_train_v0, y_test_v0 = transformer.train_test_split(dataset_v0,\n",
    "                                                                            test_size=.3,\n",
    "                                                                            shuffle=True\n",
    "                                                                           )\n",
    "\n",
    "# - dataset_v0 && - [catboost_v0] untuned classifier on dataset_v0 data set (~75% accuracy)\n",
    "dataset_v0_catboost_v0 = models._catboost_classifier(X_train_v0,\n",
    "                                                     X_test_v0,\n",
    "                                                     y_train_v0,\n",
    "                                                     y_test_v0,\n",
    "                                                     cat_features=categorical_columns_indexes\n",
    "                                                    )\n",
    "\n",
    "# - dataset_v0 && - [catboost_v1] tuned classifier on dataset_v0 data set (~__% accuracy)\n",
    "# @TODO fulfill\n",
    "\n",
    "\n",
    "# Beating Benchmark\n",
    "dataset_v1 = transformer.rename_columns(row_data)\n",
    "dataset_v1 = transformer.data_cleaning(dataset_v1)\n",
    "dataset_v1 = transformer.data_transformation(dataset_v1, features=5) # return ndarray\n",
    "X_train_v1, X_test_v1, y_train_v1, y_test_v1 = transformer.custom_split(dataset_v1,\n",
    "                                                                        test_percentage=30\n",
    "                                                                       )\n",
    "\n",
    "\n",
    "# - dataset_v1 && - [catboost_v0] untuned classifier \n",
    "dataset_v1_catboost_v0 = models._catboost_classifier(X_train_v1,\n",
    "                                                     X_test_v1,\n",
    "                                                     y_train_v1,\n",
    "                                                     y_test_v1,\n",
    "                                                     cat_features=[]\n",
    "                                                    )\n",
    "\n",
    "# - dataset_v1 && - [xgboost_classifier_v0] untuned classifier\n",
    "dataset_v1_xgboost_classifier_v0 = models._xgboost_classifier(X_train_v1,\n",
    "                                                              X_test_v1,\n",
    "                                                              y_train_v1,\n",
    "                                                              y_test_v1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7533333333333333 0.9628571428571429 0.9671428571428572\n"
     ]
    }
   ],
   "source": [
    "print(dataset_v0_catboost_v0, dataset_v1_catboost_v0, dataset_v1_xgboost_classifier_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
